/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:152: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /nobackup/sclaam/cifar-10-python.tar.gz


 82%|████████▏ | 139510784/170498071 [00:05<00:01, 30858130.31it/s]

170499072it [00:06, 26754447.06it/s]
Files already downloaded and verified
Files already downloaded and verified
Epoch 0:   0%|          | 0/362 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type       | Params
----------------------------------------
0 | model    | WideResNet | 1.1 M
1 | accuracy | Accuracy   | 0
----------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params
4.319     Total estimated model params size (MB)
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
























Epoch 0:  98%|█████████▊| 356/362 [00:51<00:00,  6.97it/s, v_num=i5ei]

Validation DataLoader 0:  40%|████      | 4/10 [00:00<00:01,  4.03it/s]
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('Epoch_FLOPS', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.























































Epoch 1:  98%|█████████▊| 356/362 [02:45<00:02,  2.15it/s, v_num=i5ei, val_loss=2.050, val_acc=0.192]
























































































Epoch 2:  97%|█████████▋| 352/362 [05:42<00:09,  1.03it/s, v_num=i5ei, val_loss=1.980, val_acc=0.224]

























































































































Epoch 3:  98%|█████████▊| 353/362 [09:47<00:14,  1.67s/it, v_num=i5ei, val_loss=1.960, val_acc=0.245]
























































































































































Epoch 4:  98%|█████████▊| 353/362 [14:54<00:22,  2.53s/it, v_num=i5ei, val_loss=1.950, val_acc=0.264]

























































































































































































Epoch 5:  97%|█████████▋| 352/362 [21:06<00:35,  3.60s/it, v_num=i5ei, val_loss=1.980, val_acc=0.248]


























































































































































































































Epoch 6:  97%|█████████▋| 352/362 [28:25<00:48,  4.85s/it, v_num=i5ei, val_loss=1.940, val_acc=0.270]






























