wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:152: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Sanity Checking: 0it [00:00, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type       | Params
----------------------------------------
0 | model    | WideResNet | 1.1 M
1 | accuracy | Accuracy   | 0
----------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params
4.319     Total estimated model params size (MB)
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.


Epoch 0:   2%|▏         | 6/356 [00:01<01:16,  4.59it/s, v_num=ymxl]
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.





























Epoch 0:  99%|█████████▉| 352/356 [01:00<00:00,  5.78it/s, v_num=ymxl]





























































Epoch 1:  99%|█████████▉| 352/356 [03:04<00:02,  1.91it/s, v_num=ymxl, val_loss=2.020, val_acc=0.227]
































































































Epoch 2: 100%|█████████▉| 355/356 [06:20<00:01,  1.07s/it, v_num=ymxl, val_loss=2.080, val_acc=0.180]


































































































































Epoch 3:  99%|█████████▉| 353/356 [10:42<00:05,  1.82s/it, v_num=ymxl, val_loss=1.790, val_acc=0.314]





































































































































































Epoch 4:  99%|█████████▉| 352/356 [16:14<00:11,  2.77s/it, v_num=ymxl, val_loss=1.730, val_acc=0.352]







































































































































































































Epoch 5:  99%|█████████▉| 352/356 [22:55<00:15,  3.91s/it, v_num=ymxl, val_loss=1.760, val_acc=0.342]










































































































































































































































Epoch 6:  99%|█████████▉| 352/356 [30:46<00:20,  5.24s/it, v_num=ymxl, val_loss=1.770, val_acc=0.318]











































































































































































































































































Epoch 7:  99%|█████████▉| 352/356 [39:45<00:27,  6.78s/it, v_num=ymxl, val_loss=1.770, val_acc=0.320]












































































































































































































































































































Epoch 8:  99%|█████████▉| 352/356 [49:55<00:34,  8.51s/it, v_num=ymxl, val_loss=1.770, val_acc=0.342]













































































































































































































































































































































Epoch 9:  99%|█████████▉| 352/356 [1:01:14<00:41, 10.44s/it, v_num=ymxl, val_loss=1.800, val_acc=0.322]


Files already downloaded and verified
Files already downloaded and verified
Testing DataLoader 0:   0%|          | 0/79 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

























Testing DataLoader 0: 100%|██████████| 79/79 [00:52<00:00,  1.51it/s]
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         val_acc            0.35120001435279846
        val_loss            1.7870304584503174
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Traceback (most recent call last):
  File "multiprocesing_test.py", line 626, in <module>
    single_train_SAM(cfg)
  File "multiprocesing_test.py", line 538, in single_train_SAM
    return 0
  File "/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/rank_zero.py", line 32, in wrapped_fn
    return fn(*args, **kwargs)
TypeError: finalize() missing 1 required positional argument: 'status'
Traceback (most recent call last):
  File "multiprocesing_test.py", line 626, in <module>
    single_train_SAM(cfg)
  File "multiprocesing_test.py", line 538, in single_train_SAM
    return 0
  File "/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/rank_zero.py", line 32, in wrapped_fn
    return fn(*args, **kwargs)
