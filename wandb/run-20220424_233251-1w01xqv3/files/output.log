
Files already downloaded and verified
wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:152: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Files already downloaded and verified
Files already downloaded and verified
Sanity Checking: 0it [00:00, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type       | Params
----------------------------------------
0 | model    | WideResNet | 1.1 M
1 | accuracy | Accuracy   | 0
----------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params
4.319     Total estimated model params size (MB)
/home/home01/sclaam/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.





























Epoch 0:  99%|█████████▉| 352/356 [00:58<00:00,  6.06it/s, v_num=xqv3]

























































Epoch 1:  99%|█████████▉| 352/356 [02:53<00:01,  2.03it/s, v_num=xqv3, val_loss=2.030, val_acc=0.213]
























































































Epoch 2:  99%|█████████▉| 352/356 [05:52<00:04,  1.00s/it, v_num=xqv3, val_loss=2.060, val_acc=0.172]
























































































































Epoch 3:  99%|█████████▉| 352/356 [09:54<00:06,  1.69s/it, v_num=xqv3, val_loss=1.840, val_acc=0.307]




















































































































































Epoch 4:  99%|█████████▉| 352/356 [14:56<00:10,  2.55s/it, v_num=xqv3, val_loss=1.810, val_acc=0.332]





















































































































































































Epoch 5:  99%|█████████▉| 352/356 [21:02<00:14,  3.59s/it, v_num=xqv3, val_loss=1.780, val_acc=0.346]




















































































































































































































Epoch 6:  99%|█████████▉| 352/356 [28:12<00:19,  4.81s/it, v_num=xqv3, val_loss=1.760, val_acc=0.350]




















































































































































































































































Epoch 7:  99%|█████████▉| 352/356 [36:24<00:24,  6.20s/it, v_num=xqv3, val_loss=1.750, val_acc=0.369]



















































































































































































































































































Epoch 8:  99%|█████████▉| 352/356 [45:40<00:31,  7.79s/it, v_num=xqv3, val_loss=1.760, val_acc=0.369]














































































































































































































































































































Epoch 9:  99%|█████████▉| 352/356 [56:02<00:38,  9.55s/it, v_num=xqv3, val_loss=1.760, val_acc=0.369]



Validation DataLoader 0:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]
Files already downloaded and verified
Files already downloaded and verified
Testing: 0it [00:00, ?it/s]
























Testing DataLoader 0: 100%|██████████| 79/79 [00:48<00:00,  1.62it/s]
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         val_acc            0.35659998655319214
        val_loss            1.7479125261306763
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Traceback (most recent call last):
  File "multiprocesing_test.py", line 626, in <module>
    single_train_SAM(cfg)
  File "multiprocesing_test.py", line 536, in single_train_SAM
    wandb_logger.finalize()
  File "/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/rank_zero.py", line 32, in wrapped_fn
    return fn(*args, **kwargs)
TypeError: finalize() missing 1 required positional argument: 'status'
Traceback (most recent call last):
  File "multiprocesing_test.py", line 626, in <module>
    single_train_SAM(cfg)
  File "multiprocesing_test.py", line 536, in single_train_SAM
    wandb_logger.finalize()
  File "/home/home01/sclaam/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/rank_zero.py", line 32, in wrapped_fn
    return fn(*args, **kwargs)
